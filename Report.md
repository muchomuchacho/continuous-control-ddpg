# Solving the Reacher environment with Deep Deterministic Policy Gradient
## Report
This document describes the solution for the Unity Reacher environment using DDPG. The algorithm used is based on the [ddpg-pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) created by Udacity.

The algorithm is able to converge and reach a return of +30 in 160 episodes and stay consistently at return of nearly 40 from there onwards.

## Learning Algorithm
Since the problem we are facing here is a `continuous space` environment one of the best options we have to solve it is by making use of one of the `Actor Critic` methods; we chose `DDPG` in our implementation. 
The `DDPG algorithm` used here is composed of two neural networks. A `Policy network`, or `Actor`, and a `Value network`, or `Critic` network. The `Actor` takes an observation and outputs an action while the `Critic` takes and observation and a corresponding action and outputs a `Q value`. The `Q value` meassures how good the action coming from the Actor is. Since the DDPG is an off-policy algorithm it makes use of a `ReplayBuffer` which stores observations later used to train the Actor and Critic models. In order to avoid convergence problems our `Agent` uses a local and a target network for each of our models similar to what we have seen before for the `DQN architecture`; the target models are updated by partially transferring the weights from the local models to the target ones, also known as `soft-updates`. The training loop will update the training parameters slowly by using a moving average. A further technique used here to promote exploration in the action space is the introduction of `noise` generated by the `Ornsteinâ€“Uhlenbeck` class; this process favours extensive initial exploration but slowly drifting towards the mean as the get further into the training process.

### Network Architecture
We have two models as explained above, and below is some detail of their composition:
##### The Actor model
* Input: `state_size` of 33 units
* Hidden layer: 200 neurons with ReLU activation
* Hidden layer: 150 neurons with ReLU activation
* Output: `action_size` with `tahn` activation

##### The Critic model
* Input: `state_size + action_size`
* Hidden layer: 400 neurons with ReLU activation and batch normalization
* Hidden layer: 300 neurons with ReLU activation
* Output: `q_value` of `1 node`

## Plot of Rewards
The agent solved the environment in 160 episodes. Once that score was reached it was consistently kept until the end of the training run whcih was set for 500 episodes.

![Plot](images/Figure_1.png)
