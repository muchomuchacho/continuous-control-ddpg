# Solving the Reacher environment with Deep Deterministic Policy Gradient
## Report
This document describes the solution for the Unity Reacher environment using DDPG. The algorithm used is based on the [ddpg-pendulum](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) created by Udacity.

The algorithm is able to converge and reach a return of +30 in 160 episodes and stay consistently at return of nearly 40 from there onwards.

## Learning Algorithm
Since the problem we are facing here is a `continuous space` environment one of the best options we have to solve it is by making use of one of the `Actor Critic` methods; we chose `DDPG` in our implementation. 
The `DDPG algorithm` used here is composed of two neural networks. A `Policy network`, or `Actor`, and a `Value network`, or `Critic` network. The `Actor` takes an observation and outputs an action while the `Critic` takes and observation and a corresponding action and outputs a `Q value`. The `Q value` meassures how good the action coming from the Actor is. Since the DDPG is an off-policy algorithm it makes use of a `ReplayBuffer` which stores observations later used to train the Actor and Critic models. In order to avoid convergence problems our `Agent` uses a local and a target network for each of our models similar to what we have seen before for the `DQN architecture`; the target models are updated by partially transferring the weights from the local models to the target ones, also known as `soft-updates`. The training loop will update the training parameters slowly by using a moving average. A further technique used here to promote exploration in the action space is the introduction of `noise` generated by the `Ornstein–Uhlenbeck` class; this process favours extensive initial exploration but slowly drifting towards the mean as the get further into the training process.

### Network Architecture
We have two models as explained above, and below is some detail of their composition:
##### The Actor model
* Input: `state_size` of 33 units
* Hidden layer: 200 neurons with ReLU activation
* Hidden layer: 150 neurons with ReLU activation
* Output: `action_size` with `tahn` activation

##### The Critic model
* Input: `state_size + action_size`
* Hidden layer: 400 neurons with ReLU activation and batch normalization
* Hidden layer: 300 neurons with ReLU activation
* Output: `q_value` of `1 node`

### Hyperparameters
Most of the hyperparameters have been tuned at some point along the multiple experiments performed in order to find the best balance between training speed and model stability. In many cases we have gone back to the original values since they proved to work quite well. One of the values found critical during the experiments was a combination of the epsilon value and the sigma value used in the ` Ornstein–Uhlenbeck` class used to calculate the standard deviation of the noise value. A combination of these two values prooved to be very effective to achieve a good balance between exploration and exploitation; as the slowly moved from a very randon approach to a more 'focused' as it went further into the training process.

| Hyperparameter | Value |
|---|---:|
| Replay buffer size | 1e6 |
| Replay batch size | 64 |
| Actor hidden units | 200, 150 |
| Critic hidden units | 400, 300 |
| Actor learning rate | 1e-3 |
| Critic learning rate | 1e-3 |
| Tau | 1e-3 |
| Gamma | 0.99 |
| Ornstein-Uhlenbeck, mu | 0 |
| Ornstein-Uhlenbeck, theta | 0.15 |
| Ornstein-Uhlenbeck, sigma | 0.1 |
| Num episodes | 500 |
| Max steps | 1000 |
| Epsilion start | 1.0 |
| Epsilon end | 0.01 |
| Epsilon decay | 0.98 |

## Plot of Rewards
The agent solved the environment in `160` episodes. Once that score was reached it was consistently above the required return until the end of the training run which was set for `500 episodes`, reaching a maximum return value of `39.72` multiple times towards the end of the training cycle.

![Plot](images/Figure_1.png)

## Ideas for Future Work
* Solve the environment with 20 Agents using D4PG.
* Solve the Crawler environment.
* Explore some hyperparameter tuning techniques such as Bayesian optimization or Grid search.
